%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom

\usepackage[sort&compress,numbers]{natbib}
\usepackage{todonotes}
\usepackage{amsmath, amsfonts}

%% macros for the manuscript.... %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\partition}{\ensuremath{\mathbf{P}}}

%% Symbols for likelihoods
\newcommand{\G}{\ensuremath{G}}
\renewcommand{\D}{\ensuremath{D}}
\renewcommand{\lhd}{\ensuremath{\mathcal{L}}}
\newcommand{\T}{\ensuremath{\mathbf{T}}}
\renewcommand{\E}{\ensuremath{\mathbf{E}}}

\newcommand{\argmax}{\ensuremath{\operatorname*{arg\,max}}}
\newcommand{\intd}{\ensuremath{\mathrm{\;d\,}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title*{Ancestral population genomics with coalescent hidden Markov models}
\author{Jade Yu Cheng and Thomas Mailund}
\institute{Jade Yu Cheng \at FIXME
	\email{name@email.address}
\and Thomas Mailund \at Bioinformatics Research Centre, Aarhus University \email{mailund@birc.au.dk}}
\maketitle

\abstract{FIXME}


\section{Introduction}

Understanding how species form and diverge is a central topic of biology and by observing emerging species today we can understand many of the genetic and environmental processes involved. Through such observations we can understand the underlying forces that drive speciation, but in order to understand how specific speciations occurred in the past, and understand the specifics of how existing species formed, we must make inference from the signals these events have left behind. The study of fossils is a powerful approach here, but not the only avenue to study past speciations; the speciation processes leave fossils in the genome of the resulting species, and through what you might call genetic archaeology we can study past events from the signals they left behind.

Over the last three decades several inference methods were developed to model speciation and estimate the timing of splits, the presence or absence of gene-flow during a split, and estimate population genetics parameters of the ancestral species, such as the effective population size. For a detailed review of these, focusing on the human/chimpanzee speciation, we refer to~\citet{Mailund:2014fyb}.

Early methods considered short aligned segments and modelled how these would be different samples from the underlying coalescence process in the ancestral species~\cite{Takahata:1995kl, Innan:2006hc, Burgess:2008kf, Rannala:2003vt, Yang:2002wz, Yang:2006eu, Yang:2010fm, Becquet:2009hta, Chen:2001dka, Wall:2003vb}. Common for these is that recombination was not explicitly modelled, with the exception of the model of~\citet{Becquet:2009hta} that allows intra-locus recombination at a cost in computational complexity. The methods considered genomic regions sufficiently short that recombination was considered unlikely and sufficiently apart that the regions could be considered independent.

Sequencing technology, however, has progressed dramatically over the last decade and now multiple full genome sequences are readily available for many related species and constructing sequences for new species is affordable for even small research groups. To fully exploit such data, models will have to explicitly consider recombination. We have to move from ancestral population genetics to \emph{genomics}.


\subsection{The sequential Markov coalescent and coalescent hidden Markov models}

Models for ancestral population genetics are based on coalescence theory~\cite{Hein:2004ta} that describes the stochastic process of how lineages of a present day sample coalesce into common ancestors back in time. The outcome of this process, when both coalescence events and recombination events are considered, is called the \emph{ancestral recombination graph} or ARG. The coalescence process with recombination was originally described as a process running backward in time, but~\citet{Wiuf:1999gua} showed that it could also be modelled as a process running along a sequence alignment. This process, however, is computationally intractable for long sequences because it requires keeping track of all local gene trees along the sequence.

Considering the coalescence process as a sequential process along a sequence alignment lead to two innovations for modelling long sequence alignments experiencing recombination: the \emph{sequential Markov coalescence} and \emph{coalescent hidden Markov models}; two ideas that while starting out from different ideas have now largely merged.

The sequential Markov coalescence was introduced by~\citet{McVean:2005hoa} who considered a coalescence process where lineages are not allowed to coalesce unless they share ancestral material. They showed that this process would be Markov when considered as a sequential process and dubbed the process the sequential Markov coalescence, SMC. Because the model originated from restrictions on which lineages could coalesce, rather than an attempt to approximate the Wiuf \& Hein model as a Markov process, it does not allow two lineages resulting from a recombination to re-coalesce unless they have first coalesced with other lineages. Consequently, when there are few lineages a large fraction of the coalescence event that would be allowed in the coalescence process are not allowed in the SMC. This was amended by~\citet{Marjoram:2006hpa} in a model named SMC$^\prime$ that matches the SMC process except for explicitly allowing these back-coalescence events. This model was further extended to allow higher order Markov dependencies in the MaCS model by \citet{Chen:2009fga}. This early work on sequential Markov coalescence models focused on simulating sequence data and not on data analysis.

Independently of the work on sequential Markov coalescent models we developed a model for inferring the speciation dates of the human-chimpanzee and human-gorilla splits in \citet{Hobolth:2007gza}. While this model was based on ideas from \citet{Wiuf:1999gua}, combined with ideas from \citet{Takahata:1995kl}, it did not explicitly model the coalescence process. Instead it simply specified a hidden Markov model with gene genealogies as hidden states and fitted the rate of changes in these to a sequence alignment. Parameters from the coalescence process were then inferred based on the fitted hidden Markov model parameters. The term \emph{coalescent hidden Markov model}, or CoalHMM, was coined in this paper.

We later constructed a model that merges the coalescent hidden Markov model from \citet{Hobolth:2007gza} with the SMC model in \citet{Dutheil:2009dta}. This model was also applied to the human-chimpanzee and human-gorilla speciation and revealed very clearly that not allowing back-coalescences in the long time periods where all sequences are in isolated species would lead to a serious underestimation of the recombination rate.

To alleviate this we developed a new model that uses continuous time Markov chains (CTMCs) to explicitly model all recombination and coalescence events possible for a pair of neighbouring nucleotides and built a CoalHMM from this in \citet{Mailund:2011dva}. As an added benefit, this model allowed us to infer parameters from a pair of sequences, where the previous methods requires three sequences and incomplete lineage sorting between them.

Independent of this, Li \& Durbin developed a CoalHMM based on the SMC for pairs of sequences, the so-called pairwise sequential Markov coalescent model PSMC~\cite{Li:2011eza}, for inferring changing effective population sizes back in time. \citet{Paul:2010iba} and \citet{Paul:2011gva} developed a model combining the SMC with a conditional sampling approach to scale the data size beyond pairs, while \citet{Rasmussen:2014cqa} developed a sampling approach with the same aim.

A number of models followed these, extending the models to consider gene-flow patterns~\cite{Steinrucken:2013kba,Mailund:2012ewa}, changing population sizes~\cite{Sheehan:2013iba,Schiffels:2014cua} or inference of recombination patters, and selection \cite{Munch:2014cba, Munch:2014cwa}, \cite{Dutheil:2015kl, Munch:2016dn}, and CoalHMMs have been used in a number of whole genome analyses \cite{Locke:2011gna, Hobolth:2011dia, Scally:2012ika, Prufer:2012ea, Miller:2012cxa, Abascal:2016cy, PradoMartinez:2013dna, Jonsson:2014fga}.

In this chapter we will present the theory underlying our approach to constructing coalescent hidden Markov models and present our current implementations of various models and how you can apply these to your own genomic analyses.


\section{The coalescent process and the hidden Markov model approximation}

The sequential Markov coalescent and coalescent hidden Markov models aim to approximate the coalescent model with recombination in order to make analysis tractable. The coalescent model of population genetics describes the ancestry of a sample of observed genes and gives probabilities to all the possible genealogies that can explain the observation. The typical formulation of the model is as a continuous time Markov chain (CTMC) that describes how the sampled genes find common ancestors as we look further and further back in time. 

A (time homogeneous) continuous time Markov chain (CTMC) is a random process $\{\,X(t) \;|\; t \geq 0 \,\}$ where the infinitesimal rate of change between states at time $t$ is given by a \emph{rate matrix} $Q$: Let $\pi(t)$ be the vector of state probabilities $\pi(t)_i = \Pr(X(t)=i)$ then $\frac{\mathrm{d}\,\pi(t)}{\mathrm{d}\,t} = \pi(t)Q$. The off-diagonal entries of $Q$ contains the rates at which the system changes state, so $Q_{i,j}$ is the rate of change from state $i$ to state $j$, and the diagonal entries are given by $Q_{i,i} = - \sum_{j\neq i} Q_{i,j}$. The probability of going from state $i$ at time $s$ to state $j$ at time $t>s$ in such a system is given by $\Pr(X(t)=j\,|\,X(s)=i) = \left[\exp\left(Q\left(t-s\right)\right)\right]_{i,j}$ and given an initial probability vector $\pi(0)$ the probability vector at time $t\geq 0$ is given by $\pi(t)=\pi(0)\exp(Qt)$.

There is a rich theory for CTMCs so expressing systems in terms of these gives us a very powerful framework to work in. The key property we will use is the way of computing the transition probabilities $\Pr(X(t)=j\,|\,X(s)=i)$ using matrix exponentiation $\exp\left(Q\left(t-s\right)\right)$. This can be done fully automatically---several algorithms exists and matrix exponentiation is implemented in many numerical software packages \cite{Moler2003}---and it greatly simplifies the mathematical modelling---see e.g.\ \citet{Hobolth:2011hl} and \citet{Andersen:2013iz} for examples of where matrix exponentiation simplifies otherwise rather complex computations.

Constructing a CTMC consists of specifying the rate matrix $Q$ and the initial probability vector $\pi(0)$. In most cases this is relatively straightforward, mathematically at least; explicitly specifying the rate matrix can be problematic when the state space is very large. When the state space is large it becomes necessary to automatically generate it and construct the rate matrix. In the remainder of this section, we describe how to automatically construct CTMCs capturing the coalescence process and how these can then be used to construct coalescent hidden Markov models.

\subsection{The coalescence process and continuous time Markov chains}

In the coalescence model, the CTMC runs backwards in time, and if we ignore for a second that lineages can undergo recombination or be part of a structured population, the only events we observe are when two lineage coalesce into a common ancestor. If we ignore the identity of the genes, and we start with $n$ genes, the states of the system can be represented by numbers from $1$ to $n$, $k=1,2,\ldots,n$, and the system will have transitions $k\to k-1$ for $k>1$ occurring with rate $c{k \choose 2}$ where $c$ is the rate at which pairs of lineages coalesce, a parameter that is determined by the size of the population the samples are taken from (the so-called ``effective population size'', $2N_e$, or its mutation-scaled version $\theta=4 N_e \mu$, see \citet{Hein:2004ta}). Any pair of lineages will coalesce with rate $c$, so with $k$ lineages there are $k \choose 2$ pairs and thus the rate of change is $c{k \choose 2}$. We start in the state $n$ and we typically run this process until we are left with a single lineage at which point we have a tree genealogy explaining the observed genes. This process lets us sample genealogies, and we can later randomly assign gene labels to leaves in sampled trees and assign mutations to edges on the trees to explain variation in the observed genes.

\todo[inline]{Make a good illustration of a coalescence tree.}

First simulating genealogies and then assigning identity to the leaves is only a valid approach if there is complete symmetry between the genes, however. Several variations on the coalescence process can break this symmetry. If the genes are sampled from a structured population, for example, the rate of coalescence will depend on which pair of lineages coalesce. Without complicating the process with population structure yet, we can preserve lineage identity in a finite state CTMC by representing states by more than the number of lineages. We will explicitly model the identity of ancestral lineages by tagging them with the genes they are ancestral to. If we let $S=\{\,1,2,\ldots,n\,\}$ denote the set of genes, then any subset $L\subseteq S$ can be thought of as an ancestral lineage, ancestral to the genes in $L$.

If we let $\partition\left(S\right)$ denote the set of partitions of a set $S$, i.e.\ the set of all sets $P=\{\,x_1,x_2,\ldots,x_k\,\}$ such that $x_i\cap x_j=\emptyset$, $x_i\neq\emptyset$, and $\bigcup_i x_i = S$, then any element $P\in\partition\left(S\right)$ corresponds to a set of ancestral lineages of $S$. A natural way to model the coalescence process as a finite state CTMC is thus to use $\partition\left(S\right)$ as the set of states. The process will start in the partition where each sample is a singleton, $\{\,\{i\}\;|\;i=1,\ldots,n\,\}$, and finish in the state where all samples have merged into the same lineage, $\{\,S\,\}$. The transitions of the CTMC will go from one partition to another and capture coalescence events. For each partition $P=\{\,x_1,x_2,\ldots,x_k\,\}$ we can pick out two lineages, $x_i$ and $x_j$, and merge them into one, $x_i\cup x_j$, to create the partition $P'=\{\,x_i\cup x_j,x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_{j-1},x_{j+1},\ldots,x_k\,\}$. In the transition system view we see this as a transition $P\to P'$. This particular transition, that merges lineages $x_i$ with $x_j$, occurs with rate $c$---the rate at which a \emph{specific} pair of lineages coalesce. Since a partition with $k$ elements will have $k \choose 2$ pairs to choose from, the rate out of any state with $k$ lineages will still be $c{k \choose 2}$, but with this model of the process we explicitly keep track of which lineages each sampled gene belongs to; at the cost of having a much larger state space, of course.

\todo[inline]{Insert Figure 1 from thesis here}

Using the CTMC formulation of the coalescence process we can easily compute the density of any given genealogy, $f(\G\,|\,\Theta)$, given the parameters of the CTMC, $\Theta$, (in the simple coalescence process we have described here, the only parameter is the coalescence rate, $\Theta = \left\{c\right\}$). Given a genealogy, it is also straightforward to get the probability of observing our data, the sampled genes, $\D$: $\Pr(\D\,|\,\G,\Theta)$. This probability depends on the mutation model and a simple approach is to sum over all possible sequences at internal nodes using standard methods such as Felsenstein's peeling algorithm \cite{Felsenstein_1981}. Then, the substitution model might contain more parameters as part of $\Theta$. In our approach, though, we use a Jukes-Cantor model and we scale time (via the coalescence rate $c$) in units of substitutions, so we have no additional parameters for mutations, but nothing prevents you from using other models.

More complex demographics scenarios can be modelled and their parameters will be part of $\Theta$, and in many applications it is these parameters that are of interest rather than the actual underlying genealogy, which is considered a nuisance parameter to be integrated out to get the likelihood:

\begin{equation}
	\label{eq:likelihood-integral}
    \lhd(\Theta\,|\,\D) = \int f(\D\,|\,\G,\Theta) f(\G\,|\,\Theta) \intd\G .
\end{equation}

This integral over all possible genealogies is generally not efficiently computable and must either be approximated through sampling approaches or by approximating the coalescence process with a simpler model where the integral \emph{can} be computed. 


\subsection{The coalescent with recombination}

When adding recombination to the coalescent process, we give genes length. Without recombination, we simply model genes as atomic entities with lineages going back in time; with recombination, the genes have a length dimension and the ancestry of genes can break up into several lineages as we move into the past.

If we model lineages as consisting of a finite length of nucleotides we can have lineage $x$ represented as $x=(x_1,x_2,\ldots,x_L)$ where each $x_i$ is a subset of the set of samples. Each $x_i$ plays the role of a single lineage when we model the process without recombination; it consists of the samples that have lineage $x$ as a common ancestor at the $i$'th locus. A state of the CTMC will consist of a set of such lineages---and to ensure that this state space is finite we require that for all lineages $x$ at least some $x_i$s are non-empty---such that the $i$'th index sets are disjunct and such that the union of all $i$'th index sets is the full set of samples. We do not require that the $i$'th index sets are partitions of the set of samples; some can be empty sets. The non-empty sets we call \emph{ancestral material}. These represent genetic material that is ancestral to some of the samples we are modelling the ancestry of. We require that all lineages we consider contain some ancestral material.

Pairs of lineages can coalesce, similar to how it was modelled without recombination, and individual lineages can recombine into two lineages. A coalescence event would take two lineages, $x=(x_1,\ldots,x_L)$ and $y=(y_1,\ldots,y_L)$, and produce the lineage $z=(x_1\cup y_1,\ldots,x_L\cup y_L)$. A recombination event would take one lineage $x=(x_1,\ldots,x_L)$ and pick a recombination point $k: 1 \leq k < L$. It would then produce two lineages $(x_1,\ldots,x_k,\emptyset,\ldots,\emptyset)$ and $(\emptyset,\ldots,\emptyset,x_{k+1},\ldots,x_L)$ where the ancestral material up to point $k$ is put in the first lineage and the ancestral material after point $k$ is put in the second lineage, and where we would restrict constructed lineages so recombination does not produce lineages without any ancestral material.


\todo[inline]{Make a good illustration of recombining lineages.}

Since this is another CTMC we can, in principle, compute the density of all genealogies, $f(\G\,|\,\Theta)$. We generally will assume that mutations at individual sites are independent, so we can compute the probability of the data conditional on the genealogy site-wise
\begin{equation}
  \Pr(\D\,|\,\G,\Theta) =
  \prod_{i=1}^L \Pr(\D_i\,|\,\G_i,\Theta)
\end{equation}
where $\D_i$ denote the variation at site $i$ and $\G_i$ the genealogy at site $i$.

In practice, however, the state space for this CTMC makes this approach intractable when it comes to integrate over all genealogies as in \eqref{eq:likelihood-integral}. While the integral in \eqref{eq:likelihood-integral} itself often requires sampling or approximations to compute, it is often doable. The larger state space in the process with recombination makes the integration infeasible for all but the smallest sequences and smallest sample sizes.


\subsection{The hidden Markov model approximation}

The sequential Markov coalescence approximates the coalescence process with recombination by assuming that the dependencies between genealogies is Markov:

\begin{equation}
  \label{eq:markov-genealogy}
  f(\G\,|\,\Theta) \approx
  f(\G_1\,|\,\Theta)\prod_{i=2}^{L}f(\G_{i}\,|\,\G_{i-1},\Theta)
  .
\end{equation}

With this approximation, the joint density of data and genealogies become
\begin{equation}
  \label{eq:coalhmm-joint-probability}
  f(\D,\G\,|\,\Theta) = 
  	f(\G_1\,|\,\Theta)
  	\prod_{i=2}^{L}f(\G_{i}\,|\,\G_{i-1},\Theta)
  	\prod_{i=1}^L \Pr(\D_i\,|\,\G_i,\Theta)
  	,
\end{equation}
which is characteristic of \emph{hidden Markov models}, stochastic models of sequences formulated as joint probabilities of a \emph{hidden} sequence $\mathbf{Z}=(Z_1,Z_2,\ldots,Z_L)$ and an \emph{observed} sequence $\mathbf{X}=(X_1,X_2,\ldots,X_L)$ where the dependencies between the hidden states, $Z_i$, is Markov and the observed states, $X_i$, are independent conditional on the hidden states, and the joint probability is
\begin{align}
  \Pr(\mathbf{X},\mathbf{Z})
    &= 
  	\Pr(Z_1)
  	\prod_{i=2}^{L}\Pr(Z_{i}\,|\,Z_{i-1})
  	\prod_{i=1}^L \Pr(X_i\,|\,Z_i)
  	\\
    \label{eq:hmm-joint-probability}
  	&=
  	\pi_{Z_1}
  	\prod_{i=2}^{L}\T_{Z_{i-1},Z_{i}}
  	\prod_{i=1}^L  \E_{Z_i,X_i}
\end{align}
where in \eqref{eq:hmm-joint-probability} $\pi$ denotes a vector of \emph{initial state probabilities}, $\T$ a matrix of \emph{transition probabilities}, and $\E$ a matrix of \emph{emission probabilities}.\footnote{The intuition is that the $\mathbf{Z}$ sequence is not observed but each $Z_i$ emits a symbol, $X_i$ that \emph{is} observed.}

The form of \eqref{eq:coalhmm-joint-probability} closely resembles \eqref{eq:hmm-joint-probability} except that the matrix representation for the hidden Markov model requires the state space to be finite, which the set of all possible local coalescent genealogies is not. The branch lengths of genealogies are real numbers, thus there are infinitely many genealogies. To approximate the coalescent with recombination we employ another approximation and discretise time. We divide the real line into $m$ time points and require that all coalescence event occur in one of those.

Constructing hidden Markov models therefore consist of constructing the initial state probability vector
\begin{equation}
  \label{eq:initial_prob}
  \pi_{\G_1} = \Pr(\G_1\,|\,\Theta),
\end{equation}
the transition probability matrix
\begin{equation}
  \label{eq:transition_prob}
  \T_{\G_{i-1},\G_i} = \Pr(\G_i\,|\,\G_{i-1},\Theta),
\end{equation}
and the emission probability matrix
\begin{equation}
  \E_{\G_i,\D_i} = \Pr(\D_i\,|\,\G_i,\Theta)
\end{equation}
to capture as well as possible under the approximation the probabilities we would see given the parameters $\Theta$. (In \eqref{eq:initial_prob} and \eqref{eq:transition_prob} we have replaced densities, $f(-)$, with probabilities, $\Pr(-)$, to indicate that we have discretised the set of genealogies).

The emission probabilities can be computed using standard substitution models and algorithms, and we will not describe this further. For the initial states and transition probabilities we get these from the joint probability of two neighbouring genealogies, $\Pr(\G_i,\G_{i+1})$ (where we assume that the Markov model in \eqref{eq:markov-genealogy} is homogeneous and this probability thus the same for all $i$). From this joint probability we can marginalise and get
\begin{equation}
  \Pr(\G_i) = \sum_{\G_{i+1}} \Pr(\G_i,\G_{i+1})
\end{equation}
and we set $\pi_{\G_1}=\Pr(\G_1)$ and 
\begin{equation}
  \T_{\G_{i-1},\G_i} = \frac{\Pr(\G_{i-1},\G_i)}{\Pr(\G_{i-1})}.
\end{equation}

The crux of the construction is thus computing $\Pr(\G_i,\G_{i+1})$, which we describe in the next section.

An important point to make first, however, is that by formulating the model as a hidden Markov model we obtain a number of efficient algorithms for parameter estimation and for decoding the most likely hidden genealogies. For example, to compute the likelihood of the model, the discretisation changes the integral from \eqref{eq:likelihood-integral} to the sum
\begin{equation}
  \label{eq:likelhood-sum}
  \lhd(\Theta\,|\,\D) = 
  \sum_{\G=(\G_1,\G_2,\ldots,\G_L)} 
  	\left[
   	  \pi_{\G_1}
  	  \prod_{i=2}^{L}\T_{\G_{i-1},\G_{i}}
  	  \prod_{i=1}^L  \E_{\G_i,\D_i}
  	\right]
\end{equation}
which in this form involves summing over a sum with an exponential number of terms as a function of $L$, but which can be computed in time $O(N^2L)$ using dynamic programming, where $N$ is the number of hidden states, i.e.\ the number of genealogies. The algorithm, known as the \emph{forward} algorithm, defines $F(g,i)$ to be the probability of being in genealogy $g$ at index $i$ and having observed $(\D_1,\ldots,\D_i)$:
\begin{equation}
  F(g,i) = \Pr(\D_1,\ldots,\D_i,\G_i=g)
  = \sum_{\G=(\G_1,\ldots,\G_{i-1})} 
  .
\end{equation}

This function can be computed recursively
\begin{equation}
  F(g,i) = 
  \begin{cases}
  	\pi_g & i = 1 \\
  	\E_{g,\D_i} \times \sum_{\G_{i-1}}\T_{\G_{i-1},g} \times F(\G_{i-1},i-1)
  	& \mathrm{otherwise}
  \end{cases}
\end{equation}
and the recursion can be replaced by dynamic programming, where we fill a table \texttt{F} with $\mathtt{F}[g,1] = \pi_g$ for all $g$ and then from $i=2$ to $i=L$ compute
\[
\mathtt{F}[g,i] = \E_{g,\D_i} \times \sum_{\G_{i-1}}\T_{\G_{i-1},g} \times \mathtt{F}[\G_{i-1},i-1]
\]
for all $g$. The \texttt{F} table has $N\times L$ entires and computing each recursive entry requires a sum over $N$ elements, giving us a running time of $O(N^2L)$. Once \texttt{F} has been filled, we can get the likelihood by summing over all final genealogies:
\begin{equation}
  \lhd(\Theta\,|\,\D) = \sum_{g} \mathtt{F}[g,L].
\end{equation}


\section{Constructing coalescent hidden Markov models}
\todo[inline]{How we compute the transition matrix from a CTMC.}


\section{Modeling demography as sequences of CTMCs}
\todo[inline]{Describe combinations of CTMCs for producing demographic models}




\section{Estimating parameters (better title)}
\todo[inline]{Optimisation algorithms}

\section{CoalHMM results (better title)}
\todo[inline]{Some simulation results}

\section{Using our CoalHMM software (better title?)}
\todo[inline]{Tutorial in how to use the software}


\bibliographystyle{spbasic}
\bibliography{references.bib}

\end{document}
